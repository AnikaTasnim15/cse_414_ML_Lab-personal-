import numpy as np
import matplotlib.pyplot as plt

m = 100

x1 = 2*np.random.rand(m,1)

x1.shape



x1


t0, t1 = 3, 5

y = t0 + t1*x1 + np.random.randn(m, 1) #linear regression eqn

# Visualize the data
plt.scatter(x1, y, color='blue', label='Data points')
plt.xlabel('House Size (x1)')
plt.ylabel('Price (y)')
plt.title('House Size vs Price')
plt.show()


x1_mean= np.mean(x1)
y_mean = np.mean(y)

# Step 2: Calculate the slope (theta_1)
numerator = np.sum((x1 - x1_mean) * (y - y_mean))
denominator = np.sum((x1 - x1_mean) ** 2)
theta_1_manual = numerator / denominator

# Step 3: Calculate the intercept (theta_0)
theta_0_manual = y_mean - (theta_1_manual * x1_mean)


# Print the results
print(f"Calculated theta_0 (Intercept): {theta_0_manual}")
print(f"Calculated theta_1 (Slope): {theta_1_manual}")


x1_test = 1.5

y_pred = theta_0_manual+theta_1_manual*x1_test
y_pred


import pandas as pd

# Create the dataset
data = {
    'town': ['monroe township', 'monroe township', 'monroe township', 'monroe township', 'monroe township',
             'west windsor', 'west windsor', 'west windsor', 'west windsor', 'robinsville', 'robinsville', 'robinsville', 'robinsville'],
    'area': [2600, 3000, 3200, 3600, 4000, 2600, 2800, 3300, 3600, 2600, 2900, 3100, 3600],
    'price': [550000, 565000, 610000, 680000, 725000, 585000, 615000, 650000, 710000, 575000, 600000, 620000, 695000]
}

# Create the DataFrame
df = pd.DataFrame(data)

df




df.town.unique()


from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()


le.fit(df.town)


le.transform(df.town)



le.fit_transform(df.town)


df2= df.copy()
df2.town = le.fit_transform(df.town)
df2


pd.get_dummies(df) #onehotencoding


pd.get_dummies(df.town, drop_first=True) #onehotencoding


df


# Apply One-Hot Encoding using pandas
df_encoded = pd.get_dummies(df, columns=['town'])

# Display the DataFrame with One-Hot Encoding applied
df_encoded.dtypes


df_encoded


	
def convert_bool_to_int(df):
    # Convert all boolean columns to int (0 and 1)
    bool_columns = df.select_dtypes(include='bool').columns
    df[bool_columns] = df[bool_columns].astype(int)
    return df

df_encoded = convert_bool_to_int(df_encoded)

# Display the updated DataFrame
print(df_encoded.dtypes)
df_encoded


X , y = df_encoded.area.values, df_encoded.price.values


df_encoded.area.values


X, y.shape


X.shape


X = X.reshape(-1,1)
X


from sklearn.preprocessing import MinMaxScaler
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

scaler = MinMaxScaler()

X = scaler.fit_transform(X)
X


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) #x is input and independent variable and y is dependent.
X_train, X_test, y_train, y_test


model = LinearRegression()
model.fit(X_train, y_train)


intercept = model.intercept_
slope = model.coef_[0]

# Display the intercept and slope
print(f"Intercept (theta_0): {intercept}")
print(f"Slope (theta_1): {slope}")


y_pred = model.predict(X_test)
y_pred



X_test.shape


model.predict(X_test)


plt.scatter(X, y, color='blue', label='Actual data')
plt.plot(X, model.predict(X), color='red', label='Regression line')
plt.xlabel('Normalized Area')
plt.ylabel('Price')
plt.title('Linear Regression: Price vs Normalized Area')
plt.legend()
plt.show()


df


X, y = df.drop('price', axis=1), df.price.values
X, y


X


X = pd.get_dummies(X , columns=['town'], drop_first=True)
X


X = X.values

X


scaler = MinMaxScaler()

# Fit the scaler and transform 'area' feature
X_normalized = scaler.fit_transform(X)
print(X_normalized)
X_train, X_test, y_train, y_test = train_test_split(X_normalized, y, test_size=0.2, random_state=42)


# Initialize the Linear Regression model
model = LinearRegression()

# Fit the model on the training data
model.fit(X_train, y_train)

# Get the model's intercept and slope
intercept = model.intercept_
slope = model.coef_[0]

# Display the intercept and slope
print(f"Intercept (theta_0): {intercept}")
print(f"Slope (theta_1): {slope}")


import matplotlib.pyplot as plt

# 3. Make predictions

y_pred = model.predict(X_test)

# Calculate and display the model's R-squared value (how well it fits the data)
r2_score = model.score(X_test, y_test)
print(f"R-squared value: {r2_score}")


df = pd.read_csv('HR_comma_sep.csv')
df


df.left.value_counts()


df.columns


df[['satisfaction_level', 'last_evaluation', 'number_project',
       'average_montly_hours', 'time_spend_company', 'Work_accident', 'left',
       'promotion_last_5years']].groupby('left').mean()


pd.crosstab(df.salary, df.left)


pd.crosstab(df.salary, df.left).plot(kind='bar')


pd.crosstab(df.Department, df.left).plot(kind='bar')


df2 = df[['satisfaction_level','average_montly_hours','promotion_last_5years','salary', 'left']].copy()
df2


salary_dummy = pd.get_dummies(df2.salary, prefix='salary', drop_first=True)
salary_dummy


df2 = pd.concat([df2, salary_dummy], axis=1)
df2


df2.drop('salary', axis=1, inplace=True)
df2


X, y = df2.drop('left', axis=1).values, df2.left.values




from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=42)


from sklearn.linear_model import LogisticRegression
model = LogisticRegression()


model.fit(X_train, y_train)


model.score(X_test, y_test)


def predict():
  satisfaction_level=float(input())
  average_montly_hours=float(input())
  promotion_last_5years=float(input())
  salary = input()
  if salary.lower=='high':
    data = [satisfaction_level, average_montly_hours, promotion_last_5years, False, False]
  elif salary.lower=='medium':
    data = [satisfaction_level, average_montly_hours, promotion_last_5years, False, True]
  else:
    data = [satisfaction_level, average_montly_hours, promotion_last_5years, True, False]

  cls={0:'Will stay', 1:'Will leave'}

  return f'The prediction is {cls[model.predict([data])[0]]}'


predict()






model.predict(np.array([[0.385, 157, 0, True, False]]))[0]



