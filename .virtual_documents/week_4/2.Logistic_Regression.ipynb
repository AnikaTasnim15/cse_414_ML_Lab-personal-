


import numpy as np
import matplotlib.pyplot as plt

# Generate a simple dataset with two features and binary target
np.random.seed(42)

# Generate features: x1, x2
x1 = np.random.rand(100, 1)  # Random feature 1
x2 = np.random.rand(100, 1)  # Random feature 2

# Simulate binary target (y), with some noise and a linear relationship
# Let's set a threshold where if the sum of x1 and x2 is greater than 1, y = 1, else y = 0
y = (x1 + x2 > 1).astype(int)

# Combine the features into a feature matrix X
X = np.concatenate([np.ones((100, 1)), x1, x2], axis=1)  # Add intercept term (1s)

# Visualize the dataset
plt.scatter(x1, x2, c=y, cmap='bwr', alpha=0.7)
plt.xlabel('Feature x1')
plt.ylabel('Feature x2')
plt.title('Generated Data for Logistic Regression')
plt.show()

print(f"First 5 samples:\n{X[:5]}")
print(f"First 5 target labels:\n{y[:5]}")


def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def cost_function(X, y, theta):
    m = len(y)  # Number of training examples
    h = sigmoid(np.dot(X, theta))  # Predicted probabilities (sigmoid of linear model)
    
    # Compute cost using log-loss
    cost = - (1/m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))
    return cost

# Initial weights (theta), all set to zero
theta_init = np.zeros(X.shape[1])

# Calculate initial cost (before training)
initial_cost = cost_function(X, y, theta_init)
print(f"Initial Cost: {initial_cost}")





# Step 1: Calculate means of features and target
x1_mean = np.mean(x1)
x2_mean = np.mean(x2)
y_mean = np.mean(y)

# Step 2: Calculate the covariance terms (for theta_1 and theta_2)
cov_x1_y = np.sum((x1 - x1_mean) * (y - y_mean))
cov_x2_y = np.sum((x2 - x2_mean) * (y - y_mean))

# Step 3: Calculate the variance terms (for theta_1 and theta_2)
var_x1 = np.sum((x1 - x1_mean) ** 2)
var_x2 = np.sum((x2 - x2_mean) ** 2)

# Step 4: Calculate theta_1 and theta_2
theta_1_manual = cov_x1_y / var_x1
theta_2_manual = cov_x2_y / var_x2

print(f"Manually calculated theta_1 (slope for x1): {theta_1_manual}")
print(f"Manually calculated theta_2 (slope for x2): {theta_2_manual}")

# Step 5: Calculate theta_0 (intercept)
theta_0_manual = y_mean - (theta_1_manual * x1_mean + theta_2_manual * x2_mean)

print(f"Manually calculated theta_0 (intercept): {theta_0_manual}")





# Step 6: Predicting for a new test case with multiple features
X_test = np.array([1, 0.8, 0.6])  # [intercept term, x1, x2]

# Apply the manually computed theta values to make a prediction
z = np.dot(X_test, [theta_0_manual, theta_1_manual, theta_2_manual])  # Linear combination
y_pred_manual = 1 / (1 + np.exp(-z))  # Apply sigmoid function

print(f"Predicted probability for the test case: {y_pred_manual}")

# Classify the predicted probability
y_pred_class = 1 if y_pred_manual >= 0.5 else 0

print(f"Predicted class for the test case: {y_pred_class}")



