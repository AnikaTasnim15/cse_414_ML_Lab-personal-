import pandas as pd

# Create the dataset
data = {
    'town': ['monroe township', 'monroe township', 'monroe township', 'monroe township', 'monroe township',
             'west windsor', 'west windsor', 'west windsor', 'west windsor', 'robinsville', 'robinsville', 'robinsville', 'robinsville'],
    'area': [2600, 3000, 3200, 3600, 4000, 2600, 2800, 3300, 3600, 2600, 2900, 3100, 3600],
    'price': [550000, 565000, 610000, 680000, 725000, 585000, 615000, 650000, 710000, 575000, 600000, 620000, 695000]
}

# Create the DataFrame
df = pd.DataFrame(data)

df


from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()

le.fit_transform(df.town)


df2= df.copy()
df2.town = le.fit_transform(df.town)
df2


# Apply One-Hot Encoding using pandas
df_encoded = pd.get_dummies(df, columns=['town'])

# Display the DataFrame with One-Hot Encoding applied
df_encoded.dtypes


def convert_bool_to_int(df):
    # Convert all boolean columns to int (0 and 1)
    bool_columns = df.select_dtypes(include='bool').columns
    df[bool_columns] = df[bool_columns].astype(int)
    return df

df_encoded = convert_bool_to_int(df_encoded)

# Display the updated DataFrame
print(df_encoded.dtypes)
df_encoded





X, y = df_encoded.area.values, df_encoded.price.values
X, y


X.shape



# Reshape the 'area' column to make it 2D (required by MinMaxScaler)
X_area = df['area'].values.reshape(-1, 1)
X_area.shape


from sklearn.preprocessing import MinMaxScaler
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split


scaler = MinMaxScaler()

# Fit the scaler and transform 'area' feature
X_normalized = scaler.fit_transform(X_area)

X_train, X_test, y_train, y_test = train_test_split(X_normalized, y, test_size=0.2, random_state=42)



# Initialize the Linear Regression model
model = LinearRegression()

# Fit the model on the training data
model.fit(X_train, y_train)

# Get the model's intercept and slope
intercept = model.intercept_
slope = model.coef_[0]

# Display the intercept and slope
print(f"Intercept (theta_0): {intercept}")
print(f"Slope (theta_1): {slope}")


import matplotlib.pyplot as plt

# 3. Make predictions

y_pred = model.predict(X_test)

# ---------------------------
# 4. Visualizing the regression line
# ---------------------------
plt.scatter(X, y, color='blue', label='Actual data')
plt.plot(X_normalized, model.predict(X_normalized), color='red', label='Regression line')
plt.xlabel('Normalized Area')
plt.ylabel('Price')
plt.title('Linear Regression: Price vs Normalized Area')
plt.legend()
plt.show()

# Calculate and display the model's R-squared value (how well it fits the data)
r2_score = model.score(X_test, y_test)
print(f"R-squared value: {r2_score}")








df


X, y = df.drop('price', axis=1), df.price.values
X, y


X


X = pd.get_dummies(X , columns=['town'])
X


X = X.values


scaler = MinMaxScaler()

# Fit the scaler and transform 'area' feature
X_normalized = scaler.fit_transform(X)
print(X_normalized)
X_train, X_test, y_train, y_test = train_test_split(X_normalized, y, test_size=0.2, random_state=42)


# Initialize the Linear Regression model
model = LinearRegression()

# Fit the model on the training data
model.fit(X_train, y_train)

# Get the model's intercept and slope
intercept = model.intercept_
slope = model.coef_[0]

# Display the intercept and slope
print(f"Intercept (theta_0): {intercept}")
print(f"Slope (theta_1): {slope}")


import matplotlib.pyplot as plt

# 3. Make predictions

y_pred = model.predict(X_test)

# Calculate and display the model's R-squared value (how well it fits the data)
r2_score = model.score(X_test, y_test)
print(f"R-squared value: {r2_score}")





df = pd.read_csv('./HR_comma_sep.csv')
df


df.left.value_counts()


df.columns


df[['satisfaction_level', 'last_evaluation', 'number_project',
       'average_montly_hours', 'time_spend_company', 'Work_accident', 'left',
       'promotion_last_5years']].groupby('left').mean()





pd.crosstab(df.salary, df.left)


pd.crosstab(df.salary, df.left).plot(kind='bar')


pd.crosstab(df.Department, df.left).plot(kind='bar')


df2 = df[['satisfaction_level','average_montly_hours','promotion_last_5years','salary', 'left']].copy()
df2


salary_dummy = pd.get_dummies(df2.salary, prefix='salary', drop_first=True)
salary_dummy


df2 = pd.concat([df2, salary_dummy], axis=1)
df2


df2.drop('salary', axis=1, inplace=True)
df2


X, y = df2.drop('left', axis=1).values, df2.left.values
X, y


from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)


from sklearn.linear_model import LogisticRegression
model = LogisticRegression()


model.fit(X_train, y_train)


model.score(X_test, y_test)


def predict():
  satisfaction_level=float(input())
  average_montly_hours=float(input())
  promotion_last_5years=float(input())
  salary = input()
  if salary.lower=='high':
    data = [satisfaction_level, average_montly_hours, promotion_last_5years, False, False]
  elif salary.lower=='medium':
    data = [satisfaction_level, average_montly_hours, promotion_last_5years, False, True]
  else:
    data = [satisfaction_level, average_montly_hours, promotion_last_5years, True, False]

  cls={0:'Will stay', 1:'Will leave'}

  return f'The prediction is {cls[model.predict([data])[0]]}'


predict()
